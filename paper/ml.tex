\section{AI and Machine Learning Contributions}\label{ml-sec}

As discussed in \Cref{automated-sec}, the ETP made extensive use of automated theorem provers in completing the primary goal of determining and then formalizing all the implications between the specified equational laws.  In contrast, we were only able to utilize modern large language models (LLMs) in a fairly limited fashion.  Such models were useful in writing initial code for the graphical user interfaces discussed in \Cref{gui-sec}, as well as performing some code autocompletion (using tools such as \emph{Github Copilot}) when formalizing an informal proof in \emph{Lean}.  In one instance, \emph{ChatGPT} was used\footnote{\url{https://chatgpt.com/share/670ce7db-8a44-800d-a5dc-8462c12eca3b}} to guess a complete rewriting system for the law $\x \op ((y \op y) \op \z) \formaleq x \op y$ \eqref{eq1659} which could then be formally verified, thus resolving all implications from this equation. However, in most of the difficult implications that resisted automated approaches, we found that LLMs did not provide useful suggestions beyond what the human participants could already propose.

On the other hand, we found that machine learning (ML) methods showed some promise of being able to heuristically predict the truth value of portions of implication graph, as we shall now discuss\footnote{For some discussion of other ML experiments performed during the project, see \url{https://leanprover.zulipchat.com/\#narrow/channel/458659-Equational/topic/Machine.20learning.2C.20first.20results}.}.

\subsection{Graph ML: Directed link prediction on the implication graph}
\label{sec:dlp}

We experimented with various Graph Neural Network (GNN) autoencoders to predict missing edges,
providing a way to estimate the truth value of unproven implications. To assess these models,
we defined three test sets focusing on edge existence, directionality, and bidirectionality.
The results give insight into how these models handle dense, directed graphs like ours.
A more detailed report follows below.


\subsubsection{Motivation}

Directed Link Prediction~\cite{Kipf2016} is a method enabling machine learning and deep learning
models to predict missing edges in a directed graph. For our implication graph, this translates to
predicting the truth values of unproven implications. This task serves as a necessary first step
for advancing in the following directions:

\begin{enumerate}
    \item \textbf{Reasoning over mathematical knowledge graphs:} Recent advancements allow language
    models to integrate information from multiple modalities. For example,
    \cite{Zhang2022, Yasunaga2022} share information between corresponding layers of Language Models (LMs)
    and Graph Neural Networks (GNNs), enabling simultaneous learning from text corpora and graph-structured
    data within the same expert domain. By leveraging both modalities, the language model can better
    \emph{structure} its knowledge and respond to complex queries. This dual learning process combines
    masked language modeling for text with link prediction on the graph, highlighting the importance
    of link prediction for robust reasoning.
    \item \textbf{Higher-order implication graphs:} Our implication graph currently represents only
    implications of the form $p \implies q$, not more complex ones like $(p \land r) \implies q$.
    Extending to such higher-order edges would likely involve connecting sets of nodes, thereby
    requiring hypergraph representations. For a systematic overview, see \cite{Kivela2014}.
    While specific hypergraph neural architectures exist~\cite{Feng2019}, we believe it is still
    conceptually important to apply Directed Link Prediction to simpler implication graphs first,
    providing insights and guiding principles that can anticipate challenges in higher-order graph
    representation learning.
\end{enumerate}

\subsubsection{Data}

We used \href{https://github.com/teorth/equational_theories/blob/main/data/2024-10-20-edge_list.csv.zip}{this edge list},
generated on October 20, 2024, with the following commands:

\begin{verbatim}
    lake exe extract_implications outcomes > data/tmp/outcomes.json
    scripts/generate_edgelist_csv.py
\end{verbatim}

The structure of the implication graph is summarized below:

\begin{verbatim}
    graph_summary = {
        "total_nodes": 4694,
        "total_directed_edges": 8178279,
        "edge_density_percentage": 37,  # % of possible edges that exist
        "bidirectional_edges": 2475610,
        "bidirectional_percentage": 30  # % of all edges that are bidirectional
    }
\end{verbatim}

Below is a summary of the edge types in the graph:

\begin{verbatim}
    edge_counts = {
        "explicit_conjecture_false": 92,
        "explicit_proof_false": 582316,
        "explicit_proof_true": 10657,
        "implicit_conjecture_false": 142,
        "implicit_proof_false": 13272681,
        "implicit_proof_true": 8167622,
        "unknown": 126
    }
\end{verbatim}

Edges are labeled according to the following scheme:

\begin{verbatim}
    edge_labels = {
        "implicit_proof_true": 1,
        "explicit_proof_false": 0,
        "implicit_proof_false": 0,
        "explicit_proof_true": 1,
        "explicit_conjecture_false": 0,
        "implicit_conjecture_false": 0,
        "unknown": 0
    }
\end{verbatim}

The \texttt{unknown} class contains a very small number of edges (126), so their impact on the
training phase is expected to be negligible. Future approaches might address this class by
excluding \texttt{unknown} edges from the training set.

\subsubsection{Methods}

Consider a directed graph $G = (V, E)$ where $E = \{(u,v) \mid u, v \in V\}$ is the edge set and
$|V| = n$. We assume that each node is associated with a feature vector, resulting in an
$X \in \mathbb{R}^{n \times f}$ feature matrix.

We define the existing edges $(a, b) \in E$ as \emph{positives} and the non-existing edges
$(c, d) \notin E$ as \emph{negatives}.

Intuitively, performing Directed Link Prediction (DLP) on $G$ involves randomly splitting $E$
into three disjoint sets: $E_{\text{train}}$, $E_{\text{val}}$, and $E_{\text{test}}$, such that:

\begin{itemize}
    \item $E_{\text{train}}$ is the training set,
    \item $E_{\text{val}}$ is the validation set,
    \item $E_{\text{test}}$ is the test set,
    \item and $E = E_{\text{train}} \dot{\cup} E_{\text{val}} \dot{\cup} E_{\text{test}}$.
\end{itemize}

The model then learns from $G_{\text{train}} = (V, E_{\text{train}})$ to map the topological and
feature-related information of two nodes $u$ and $v$ to a probability $p_{uv}$ that
$(u, v) \in E_{\text{test}}$.

However, this setup presents two key issues among others:

\begin{enumerate}
    \item The model learns only from \emph{positives}, so it cannot recognize \emph{negatives}.
    \item The model is evaluated only on \emph{positives}, preventing us from measuring its ability
    to identify \emph{negatives}.
\end{enumerate}

To address these limitations, we adopted the setup proposed in~\cite{Salha2019}.
Specifically, we redefined $E_{\text{train}}$ to $E_{\text{train}}^p$ (\emph{positives}) and introduced:

\[
E_{\text{train}} = E_{\text{train}}^p \dot{\cup} E_{\text{train}}^n
\]

where $E_{\text{train}}^n$ includes all possible \emph{negatives} in $G_{\text{train}} = (V, E_{\text{train}}^p)$.
The model is now required to predict the non-existence of edges in $E_{\text{train}}^n$.

Similarly, if we redefine $E_{\text{test}}$ as follows:

\[
E_{\text{test}} = E_{\text{test}}^p \dot{\cup} E_{\text{test}}^n
\]

where $E_{\text{test}}^n$ is a \emph{random} sample of \emph{negatives}, the model's evaluation
would fail to capture two crucial aspects:

\begin{enumerate}
    \item The model's ability to distinguish $(u,v)$ from $(v,u)$ for all $(u,v) \in E_{\text{test}}^p$.
    \item The model's ability to identify bi-implications.
\end{enumerate}

These limitations arise from the random selection of negative edges in $E_{\text{test}}^n$.
To address this, we define three distinct test sets: $E_{\text{test}}^G$, $E_{\text{test}}^D$,
and $E_{\text{test}}^B$, to evaluate different facets of the modelâ€™s performance:

\begin{itemize}
    \item \textbf{General Test Set} ($E_{\text{test}}^G$):
    Here, $E_{\text{test}} = E_{\text{test}}^p \dot{\cup} E_{\text{test}}^n$, where $E_{\text{test}}^n$
    is a random sample of non-existent edges with the same cardinality as $E_{\text{test}}^p$.
    This set assesses the model's ability to detect the presence of edges, regardless of direction.
    A model that cannot distinguish edge direction may still perform well on this set, highlighting
    the need for the following two additional test sets.
    \item \textbf{Directional Test Set} ($E_{\text{test}}^D$):
    Defined as $E_{\text{test}}^{\text{up}} \dot{\cup} \tilde{E}_{\text{test}}^{\text{up}}$,
    where $E_{\text{test}}^{\text{up}}$ consists of unidirectional edges in $E_{\text{test}}^p$,
    and $\tilde{E}_{\text{test}}^{\text{up}}$ contains their reverses (negatives by construction).
    This set evaluates the model's ability to recognize edge direction, making it suitable for
    assessing direction-sensitive models.
    \item \textbf{Bidirectional Test Set} ($E_{\text{test}}^B$):
    Defined as $E_{\text{test}}^{\text{bp}} \dot{\cup} E_{\text{B}}^{\text{n}}$,
    where $E_{\text{test}}^{\text{bp}}$ contains one direction of all bidirectional edges in $E_{\text{test}}^p$,
    and $E_{\text{B}}^{\text{n}} \subset \tilde{E}$ includes a subset of their reverses.
    This set evaluates the model's ability to identify bi-implications, as each edge in $E_{\text{test}}^B$
    has a reverse that is positive, but only half are bidirectional in practice.
\end{itemize}

We tested the following models:

\begin{itemize}
    \item \textbf{GAE}~\cite{Kipf2016}
    \item \textbf{Gravity-GAE}~\cite{Salha2019}
    \item \textbf{Source/Target-GAE}~\cite{Salha2019}
    \item \textbf{DiGAE}~\cite{Kollias2022}
    \item \textbf{MagNet}~\cite{Zhang2021}
\end{itemize}

All these models are graph-based autoencoders with distinct encoder-decoder architectures.
Notably, GAE is the only model structurally unable to differentiate edge directions.
Each model outputs the probability that an ordered pair of nodes has a directed edge between them,
with nodes represented using one-hot encodings as features.

We trained the models using Binary Cross Entropy as the loss function, balancing the contribution
of positive and negative edges through re-weighting. On the \emph{General} test set, we evaluated
the following metrics:

\begin{itemize}
    \item \textbf{AUC} (Area Under the ROC Curve): Measures the probability that the model ranks a
    random positive edge higher than a random negative edge. Higher values indicate better
    discrimination between positive and negative edges.
    \item \textbf{AUPRC} (Area Under Precision-Recall Curve): Assesses model performance,
    particularly in cases of class imbalance. AUPRC is more robust to imbalanced data than AUC.
    \item \textbf{Hits@K}: Evaluates the fraction of times a positive edge is ranked within the
    top $K$ positions among personalized negative samples~\cite{Li2023}. Briefly, given a positive
    edge, its $M$ personalized negative samples are $M$ negative edges with the same head but
    different tails. We calculate Hits@K for $K = 1, 3, 10$ to assess ranking quality, and
    set $M = 100$. Therefore, Hits@K = 0.1 means that on average, the model ranks a positive
    edge within the highest-ranked $K$ personalized negatives $10\%$ of the time.
    \item \textbf{MRR} (Mean Reciprocal Rank): Computes the average reciprocal rank of positive
    edges among their personalized negative samples~\cite{Li2023} (the same as those used for Hits@K)
    providing an overall measure of ranking accuracy. For instance, $MRR = 0.1$ means that on average,
    the model ranks a positive edge as $10^{\text{th}}$ among $M$ personalized negative samples.
\end{itemize}

Each metric ranges from 0 to 1, with higher values reflecting improved performance.
Based on prior work, we expect AUC and AUPRC scores to approach 1, while MRR and Hits@K often
yield values around 0.15 for similar undirected tasks~\cite{Li2023}. \emph{Directional} and
\emph{Bidirectional} performances were evaluated using only AUC and AUPRC, since Hits@K and MRR
are hardly definable in those scenarios. The number of training epochs was optimized through
Early Stopping on the \emph{General} validation set performance (given by the sum of AUC and AUPRC).

\subsubsection{Results}

The results below represent average performance over six random splits of $E_{\text{train}}$,
$E_{\text{val}}$, and $E_{\text{test}}$ while keeping the model's seed fixed for fair comparison.
The \emph{training / validation / test} split proportions are:

\begin{itemize}
    \item $85 / 5 / 10$ for unidirectional edges,
    \item $65 / 15 / 30$ for bidirectional edges.
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lcccccccccc}
\hline
\textbf{Model} & \textbf{G\_ROC\_AUC} & \textbf{G\_AUPRC} & \textbf{G\_Hits@1} & \textbf{G\_Hits@3} & \textbf{G\_Hits@10} & \textbf{G\_MRR} & \textbf{D\_ROC\_AUC} & \textbf{D\_AUPRC} & \textbf{B\_ROC\_AUC} & \textbf{B\_AUPRC} \\
\hline
\texttt{gae}              & $0.8484 \pm 9 \times 10^{-4}$  & $0.8558 \pm 6 \times 10^{-4}$   & $6 \times 10^{-5} \pm 4 \times 10^{-5}$ & $6 \times 10^{-5} \pm 4 \times 10^{-5}$ & $0.0001 \pm 4 \times 10^{-5}$ & $0.0165 \pm 2 \times 10^{-4}$ & $0.5 \pm 0$          & $0.5 \pm 0$          & $0.941 \pm 5 \times 10^{-3}$    & $0.964 \pm 3 \times 10^{-3}$    \\
\texttt{gravity\_gae}      & $0.9806 \pm 3 \times 10^{-4}$  & $0.9753 \pm 4 \times 10^{-4}$   & $0.069 \pm 6 \times 10^{-3}$ & $0.101 \pm 5 \times 10^{-3}$ & $0.17 \pm 3 \times 10^{-3}$   & $0.112 \pm 5 \times 10^{-3}$  & $0.9958 \pm 1 \times 10^{-4}$   & $0.9874 \pm 2 \times 10^{-4}$   & $0.99717 \pm 4 \times 10^{-5}$  & $0.99431 \pm 7 \times 10^{-5}$  \\
\texttt{sourcetarget\_gae} & $0.99976 \pm 1 \times 10^{-5}$ & $0.999736 \pm 8 \times 10^{-6}$ & $0.077 \pm 4 \times 10^{-3}$ & $0.147 \pm 7 \times 10^{-3}$ & $0.279 \pm 9 \times 10^{-3}$  & $0.152 \pm 5 \times 10^{-3}$  & $0.999982 \pm 1 \times 10^{-6}$ & $0.999983 \pm 1 \times 10^{-6}$ & $0.999989 \pm 2 \times 10^{-6}$ & $0.999987 \pm 3 \times 10^{-6}$ \\
\texttt{mlp\_gae}          & $0.99315 \pm 1 \times 10^{-5}$ & $0.99409 \pm 1 \times 10^{-5}$  & $0.181 \pm 7 \times 10^{-3}$ & $0.299 \pm 7 \times 10^{-3}$ & $0.53 \pm 2 \times 10^{-3}$   & $0.289 \pm 6 \times 10^{-3}$  & $0.99671 \pm 2 \times 10^{-5}$  & $0.9973 \pm 1 \times 10^{-5}$   & $0.99692 \pm 2 \times 10^{-5}$  & $0.99736 \pm 2 \times 10^{-5}$  \\
\texttt{digae}            & $0.9978 \pm 3 \times 10^{-4}$  & $0.998 \pm 3 \times 10^{-4}$    & $0.035 \pm 6 \times 10^{-3}$ & $0.068 \pm 1 \times 10^{-2}$ & $0.18 \pm 2 \times 10^{-2}$   & $0.091 \pm 1 \times 10^{-2}$  & $0.9991 \pm 2 \times 10^{-4}$   & $0.9993 \pm 2 \times 10^{-4}$   & $0.9994 \pm 1 \times 10^{-4}$   & $0.9995 \pm 1 \times 10^{-4}$   \\
\texttt{magnet}           & $0.989 \pm 1 \times 10^{-4}$   & $0.99076 \pm 3 \times 10^{-5}$  & $0.151 \pm 1 \times 10^{-2}$ & $0.26 \pm 2 \times 10^{-2}$  & $0.38 \pm 2 \times 10^{-2}$   & $0.24 \pm 1 \times 10^{-2}$   & $0.9962 \pm 1 \times 10^{-3}$   & $0.9969 \pm 6 \times 10^{-4}$   & $0.9976 \pm 4 \times 10^{-4}$   & $0.9979 \pm 2 \times 10^{-4}$   \\
\hline
\end{tabular}
\caption{Results for various graph autoencoder models.}
\end{table}

\subsubsection{Discussion}

We observe consistently high \emph{General} AUC and AUPRC scores, close to 1.
These high values are expected, as similar neural architectures have demonstrated strong performance
in graphs of comparable size~\cite{Kipf2016}. The high ratio of existing to non-existing edges in the
implication graph (approximately 37\%) likely contributes to the near-perfect \emph{General} AUC and
AUPRC scores. For context, benchmark datasets such as Cora and Citeseer
(e.g., \href{https://github.com/deezer/gravity_graph_autoencoders/tree/master/data}{directed}
and \href{https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html}{undirected})
contain fewer than 1\% of all possible edges.

Interestingly, the GAE model, though structurally unable to distinguish edge direction, performs well
on the \emph{General} task (if we consider AUC and AUPRC only). This experimentally confirms the need
for including \emph{Directional} and \emph{Bidirectional} test sets, which allow comprehensive
evaluation across all facets of Directed Link Prediction (DLP).

All other models demonstrate high AUC and AUPRC scores across the \emph{General}, \emph{Directional},
and \emph{Bidirectional} test sets, indicating strong predictive capabilities even when accounting for
directionality and bidirectionality.

Notably, the \texttt{mlp\_gae} and \texttt{magnet} models also achieve competitive scores in MRR and
Hits@K metrics.

\subsubsection{Conclusions}

We evaluated the performance of six different graph autoencoders on a Directed Link Prediction (DLP) task.
By adopting a multi-task evaluation framework, we assessed the models comprehensively across various
aspects of DLP. All models demonstrated strong performance on AUC and AUPRC metrics, with some also
achieving high scores on MRR and Hits@K.

Node features were represented using one-hot encodings, meaning that the models received no explicit
information about the equations represented by the nodes. Instead, they relied entirely on the
topological structure encoded during training. This approach aligns with previous research suggesting
that one-hot encodings can promote asymmetric embeddings~\cite{Salha2019}. However, future experiments
could explore alternative representations, such as encoding the equations with text-based embeddings
like Word2Vec, to potentially enhance the models' understanding of the nodes' semantic content.

In summary, our findings highlight the adaptability and robustness of graph autoencoders for DLP
tasks in dense, directed graphs. This approach not only demonstrates robustness in predicting directed
links but also suggests promising potential for future improvements through enhanced feature
representations, thereby advancing the capabilities of link prediction in complex mathematical
knowledge graphs.
